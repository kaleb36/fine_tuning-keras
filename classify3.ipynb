{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b35f827-6d3c-472e-98e9-06714d846b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 17:58:15.299197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 17:58:16.497248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 17:58:32.558196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-17 17:58:32.692542: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    \"/mnt/h/deep2/kal_classify/train\",\n",
    "    image_size=(180,180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    \"/mnt/h/deep2/kal_classify/validation\",\n",
    "    image_size=(180,180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    \"/mnt/h/deep2/kal_classify/test\",\n",
    "    image_size=(180,180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7288e8-180b-49dd-9c20-0a2f3549d6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - ETA: 0s - loss: 8.0975 - accuracy: 0.9548"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 18:11:42.727560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [4000]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-11-17 18:11:42.727944: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [4000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 983s 3s/step - loss: 8.0975 - accuracy: 0.9548 - val_loss: 10.1067 - val_accuracy: 0.9542\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 1031s 3s/step - loss: 3.2420 - accuracy: 0.9789 - val_loss: 5.2997 - val_accuracy: 0.9690\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 1022s 3s/step - loss: 1.7991 - accuracy: 0.9853 - val_loss: 5.6895 - val_accuracy: 0.9675\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 1136s 4s/step - loss: 1.4776 - accuracy: 0.9891 - val_loss: 3.5603 - val_accuracy: 0.9735\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 1124s 4s/step - loss: 0.7222 - accuracy: 0.9926 - val_loss: 3.9535 - val_accuracy: 0.9758\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 1150s 4s/step - loss: 0.5622 - accuracy: 0.9934 - val_loss: 3.8563 - val_accuracy: 0.9753\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 1150s 4s/step - loss: 0.5864 - accuracy: 0.9926 - val_loss: 7.4826 - val_accuracy: 0.9595\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 1126s 4s/step - loss: 0.3338 - accuracy: 0.9950 - val_loss: 4.8993 - val_accuracy: 0.9680\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 1063s 3s/step - loss: 0.2948 - accuracy: 0.9960 - val_loss: 9.1462 - val_accuracy: 0.9567\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 1043s 3s/step - loss: 0.2976 - accuracy: 0.9965 - val_loss: 4.5744 - val_accuracy: 0.9720\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 978s 3s/step - loss: 0.2301 - accuracy: 0.9967 - val_loss: 4.1632 - val_accuracy: 0.9750\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 1060s 3s/step - loss: 0.1859 - accuracy: 0.9972 - val_loss: 4.8806 - val_accuracy: 0.9755\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 1071s 3s/step - loss: 0.1054 - accuracy: 0.9979 - val_loss: 4.3582 - val_accuracy: 0.9762\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 1066s 3s/step - loss: 0.1571 - accuracy: 0.9971 - val_loss: 4.6397 - val_accuracy: 0.9760\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 1083s 3s/step - loss: 0.1643 - accuracy: 0.9980 - val_loss: 4.6571 - val_accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#convolution base\n",
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "#setting convolution base to False to freeze the weights during training\n",
    "conv_base.trainable = False \n",
    "\n",
    "#our model\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = keras.applications.vgg16.preprocess_input(inputs) #applying value scaling\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"rmsprop\",\n",
    "             metrics=[\"accuracy\"])\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75670e90-5838-4487-8bb7-c2a138d1070a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaltf",
   "language": "python",
   "name": "kaltf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
